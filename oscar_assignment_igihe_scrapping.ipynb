{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byiringiroscar/NLP_FELLOWSHIP/blob/main/oscar_assignment_igihe_scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYsT11VGSX8E"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "====   here we are going to use api for wayback machine to help us to get all daily snapshoot"
      ],
      "metadata": {
        "id": "DR9dTWL4rh0N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oTo4TVTPTC4D"
      },
      "outputs": [],
      "source": [
        "snap = []\n",
        "err = []\n",
        "for month in range(1,11):\n",
        "    for day in range(1,32):\n",
        "        link = requests.get(url='http://archive.org/wayback/available?url=igihe.com&timestamp=2022{:02d}{:02d}'.format(month, day))\n",
        "        try:\n",
        "            if link.json()['archived_snapshots']['closest']['available']:\n",
        "              #we store each snapshot link into snap list\n",
        "                snap.append(link.json()['archived_snapshots']['closest']['url'])\n",
        "        except KeyError:\n",
        "            err.append(link.json())\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XFttToNJUJWO",
        "outputId": "209339fe-14d3-45bc-b0c4-9ed35840d58f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "301"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(snap)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "===== downside we are going to remove the duplicate in all snappshoot captured"
      ],
      "metadata": {
        "id": "Ut-zoSLvr0RZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DNhp-2TdURKf"
      },
      "outputs": [],
      "source": [
        "snap_set  = set(snap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6CMMbNlqUXdh",
        "outputId": "bd32963e-d03e-4913-c9b1-556c4adc6548"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "195"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "snap_r = list(snap_set)\n",
        "len(snap_r)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "==== here we are going to BeautifulSoup to get all content in all snapshoot"
      ],
      "metadata": {
        "id": "N0RtOpSrsFob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0m2_0i7Uef5"
      },
      "outputs": [],
      "source": [
        "#Fetch all the titles and their link from wayback archivr\n",
        "\n",
        "links_art = []\n",
        "\n",
        "for lin in snap_r:\n",
        "  content = requests.get(lin).content #pass each snapshot link to the requests\n",
        "  soup =  BeautifulSoup(content, \"html.parser\") #pass each snapshot content html parser\n",
        "  articles = soup.find_all('span', class_=\"homenews-title\") #look for all title in the snapshot\n",
        "  links_art.append(articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F0HCiEJYGdU"
      },
      "outputs": [],
      "source": [
        "len(list(snap_r))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "==== downside we are going to put all links in one side with their title in order to be zipped"
      ],
      "metadata": {
        "id": "1Akp7aA0sRuu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew-hOTe0XVjb"
      },
      "outputs": [],
      "source": [
        "#Actual link\n",
        "snapshot_link = []\n",
        "title_ = []\n",
        "\n",
        "#Loop all the 184 snapshots\n",
        "for index, snapshot in enumerate(list(snap_r)):\n",
        "  #In each title assign it's snapshop prefix link\n",
        "  for title in links_art[index]:\n",
        "    snapshot_link.append(snapshot)\n",
        "    title_.append(str(title.find('a')['href']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSw8CK9CXnOH"
      },
      "outputs": [],
      "source": [
        "len(snapshot_link)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=== here we are creating new dataframe with title and title_link"
      ],
      "metadata": {
        "id": "bw9ExqWHscnE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW4e0KU_bCkC"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(list(zip(snapshot_link, title_)), columns=['Prefix', 'Title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJJd_u9GbHKh"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HByUy-8JbMSp"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=== here we are removing duplication from our list"
      ],
      "metadata": {
        "id": "uGyiqqvnsuYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(['Title'])"
      ],
      "metadata": {
        "id": "LhrKjsm6qWba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s69-7K8OblQB"
      },
      "outputs": [],
      "source": [
        "link = df['Prefix'][0] + df['Title'][0]\n",
        "\n",
        "content = requests.get(link).content\n",
        "\n",
        "raw_content = BeautifulSoup(content, 'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txWL4AFEbvTm"
      },
      "outputs": [],
      "source": [
        "body = raw_content.find_all('div', class_=\"fulltext margintop10\") \n",
        "\n",
        "text = ''\n",
        "par_ = BeautifulSoup(str(body)).find_all('p')\n",
        "\n",
        "for line in par_:\n",
        "  text = text + '\\n' + line.get_text()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZKoZ66McA0x"
      },
      "outputs": [],
      "source": [
        "def get_text(link):\n",
        "  content = requests.get(link).content\n",
        "  raw_content = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "  body = raw_content.find_all('div', class_=\"fulltext margintop10\") \n",
        "\n",
        "  text = ''\n",
        "  par_ = BeautifulSoup(str(body)).find_all('p')\n",
        "\n",
        "  for line in par_:\n",
        "    text = text + '\\n' + line.get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M2zWL61cOWK"
      },
      "outputs": [],
      "source": [
        "def get_article_text(link):\n",
        "  content = requests.get(link).content\n",
        "  body = BeautifulSoup(content, 'html.parser').find_all('div', class_='fulltext margintop10')\n",
        "\n",
        "  text = ''\n",
        "  try:\n",
        "    par_ = BeautifulSoup(str(body)).find_all('p')\n",
        "\n",
        "\n",
        "    for line in par_:\n",
        "      text = text + '\\n' + line.get_text()\n",
        "  except AttributeError:\n",
        "    pass\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a1idH10McVYT"
      },
      "outputs": [],
      "source": [
        "articles = []\n",
        "\n",
        "for index, article in enumerate(df['Title']):\n",
        "  link =df['Prefix'][index]+article\n",
        "  txt = get_article_text(link)\n",
        "  articles.append(txt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "=== here we are going to take all content of each link then save it in text"
      ],
      "metadata": {
        "id": "LDtOtOACs9wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_text_file(index):\n",
        "  link = df['Prefix'][index] + df['Title'][index]\n",
        "\n",
        "  content = requests.get(link).content\n",
        "\n",
        "  raw_content = BeautifulSoup(content, 'html.parser')\n",
        "  body = raw_content.find_all('div', class_=\"fulltext margintop10\") \n",
        "\n",
        "  text = ''\n",
        "  par_ = BeautifulSoup(str(body)).find_all('p')\n",
        "\n",
        "  for line in par_:\n",
        "    text = text + '\\n' + line.get_text()\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "y7iRkfqVbpUw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index in range(len(df)):\n",
        "  new_t = get_all_text_file(index)\n",
        "  y = open('/content/drive/MyDrive/NLP Fellowship/huzalab_doc/week1/content_file.txt', 'w+',  encoding='utf-8')\n",
        "  y.write(new_t+' \\n')"
      ],
      "metadata": {
        "id": "G7psK9MwqoR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5nCEZEifrgb5"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1wYPxmDkYuOMZWd2Ls1OOS4BGKKJqcJm2",
      "authorship_tag": "ABX9TyMnJWHxig+f0hewUU98hkgT",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}