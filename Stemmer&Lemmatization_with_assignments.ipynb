{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byiringiroscar/NLP_FELLOWSHIP/blob/main/Stemmer%26Lemmatization_with_assignments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "*A stemming algorithm, a procedure to reduce all words with the same\n",
        "stem to a common form, is useful in many areas of computational linguistics and information-retrieval work.* ~ Lovin,1968\n",
        "\n",
        "Examples of Stemmers include:\n",
        "\n",
        "\n",
        "1.   PorterStemmer\n",
        "2.   SnowballStemmer\n",
        "3. LancasterStemmer\n",
        "4. RegexStemmer\n",
        "\n"
      ],
      "metadata": {
        "id": "in_28geDz1Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "OYQ40jZx-Awx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe038a1-af75-465c-b5ba-8504511cf2bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "== here down we are going to stem by using actuall text"
      ],
      "metadata": {
        "id": "jxV89Hm37Fhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5TtpNnE8Uxi",
        "outputId": "5c45c9d3-db39-476d-80d4-2aab2d2171d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here we are stemming by using text"
      ],
      "metadata": {
        "id": "LRSIxy4x_Djy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer,LancasterStemmer,RegexpStemmer,WordNetLemmatizer"
      ],
      "metadata": {
        "id": "CVWwi4R03flR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "sentense = \"Edureka is the best place to learn about the new trending technologies online\"\n",
        "porter.stem(sentense)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RzuU313s3yva",
        "outputId": "5628f22b-811d-4475-8a77-353fcdab1d04"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'edureka is the best place to learn about the new trending technologies onlin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentense = \"Edureka is the best place to learn about the new trending technologies online\"\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "def stemSentence(sentence):\n",
        "  tokens_words = word_tokenize(sentence)\n",
        "  stem_sentence = []\n",
        "  for word in tokens_words:\n",
        "    stem_sentence.append(porter.stem(word))\n",
        "    stem_sentence.append(\" \")\n",
        "  return \"\".join(stem_sentence)\n",
        "\n",
        "x = stemSentence(sentense)\n",
        "x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Zx5YO1FH40-B",
        "outputId": "706b62ea-6743-4a88-884f-fd0f66df49cd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'edureka is the best place to learn about the new trend technolog onlin '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/NLP Fellowship/huzalab_doc/week3/file_tokenization.txt\")\n",
        "my_lines_list = file.readlines()\n",
        "print(my_lines_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJCJUq3-_Rix",
        "outputId": "ee07657f-9786-4841-a331-c78ff0024578"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here we are trying our best to be the one who can solve it\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def StemSentence_document(sentence_doc):\n",
        "  tokens_words = word_tokenize(sentence_doc)\n",
        "  stem_sentence = []\n",
        "  for word in tokens_words:\n",
        "    stem_sentence.append(porter.stem(word))\n",
        "    stem_sentence.append(\" \")\n",
        "  return \"\".join(stem_sentence)\n",
        "x = StemSentence_document(my_lines_list[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C0CJ0ehH_q0A",
        "outputId": "4a761c60-3164-4672-8a71-33c58fb8261d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'here we are tri our best to be the one who can solv it '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save the data from text with stemmed"
      ],
      "metadata": {
        "id": "J0iiBq6yBGIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stem_file = open(\"/content/drive/MyDrive/NLP Fellowship/huzalab_doc/week3/saveStem_doc.txt\", mode=\"a+\", encoding=\"utf-8\")\n",
        "for line in my_lines_list:\n",
        "  stem_sentence = StemSentence_document(line)\n",
        "  stem_sentence\n",
        "  stem_file.write(stem_sentence)\n",
        "stem_file.close"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJAqYniuBK3V",
        "outputId": "e15c9b4a-646e-4ffc-ea0f-a2b3626e6acd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function TextIOWrapper.close()>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # === Lemmantization pract"
      ],
      "metadata": {
        "id": "bDJq822kC2i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "wordnet_lemmantizer = WordNetLemmatizer()\n",
        "\n",
        "sentence_1 =\"He was running and eating at the same time. he has bad habit of swimming after playing long hours in the sun\""
      ],
      "metadata": {
        "id": "c27z_vhrDAb_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuations = \"?:!.,;\"\n",
        "sentense_words1 = nltk.word_tokenize(sentence_1)\n",
        "for word in sentense_words1:\n",
        "  if word in punctuations:\n",
        "    sentense_words1.remove(word)\n",
        "# sentense_words1\n",
        "print(\"{0:20}{1:20}\".format(\"word\", \"Lemma\"))\n",
        "for word in sentense_words1:\n",
        "  print(\"{0:20}{1:20}\".format(word, wordnet_lemmantizer.lemmatize(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMWX_UCBDYJA",
        "outputId": "9eae24d2-c32d-4732-ee32-939af7a4f3e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word                Lemma               \n",
            "He                  He                  \n",
            "was                 wa                  \n",
            "running             running             \n",
            "and                 and                 \n",
            "eating              eating              \n",
            "at                  at                  \n",
            "the                 the                 \n",
            "same                same                \n",
            "time                time                \n",
            "he                  he                  \n",
            "has                 ha                  \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swimming            \n",
            "after               after               \n",
            "playing             playing             \n",
            "long                long                \n",
            "hours               hour                \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "sun                 sun                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# add context to lemmitizer by adding pos parameter"
      ],
      "metadata": {
        "id": "beBT8i7nTJ0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WwD-oXkiBEdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in sentense_words1:\n",
        "  print(\"{0:20}{1:20}\".format(word, wordnet_lemmantizer.lemmatize(word, pos=\"v\")))"
      ],
      "metadata": {
        "id": "XX5tWk4oTSlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# here we are going to steam by using documents"
      ],
      "metadata": {
        "id": "49r2XtqZ_Kk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PorterStemmer\n",
        "Designed and built by Martin Porter in 1980\n",
        "\n",
        "Takes Five steps each with its own mapping rules. Easy and fast\n"
      ],
      "metadata": {
        "id": "pP3w12Sy0QDd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Edofg-4JoDXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3763b65c-6fad-488d-8777-b201cbfa6401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "friend ---> friend\n",
            "friendship ---> friendship\n",
            "friends ---> friend\n",
            "friendships ---> friendship\n",
            "generate ---> gener\n",
            "generates ---> gener\n",
            "generating ---> gener\n",
            "general ---> gener\n",
            "generally ---> gener\n",
            "generic ---> gener\n",
            "generically ---> gener\n",
            "generous ---> gener\n",
            "generously ---> gener\n",
            "went ---> went\n",
            "ate ---> ate\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer,LancasterStemmer,RegexpStemmer,WordNetLemmatizer\n",
        "words = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"generate\",\"generates\",\"generating\",\"general\",\"generally\",\"generic\",\"generically\",\"generous\",\"generously\",\"went\",\"ate\"]\n",
        "Porter = PorterStemmer()\n",
        "\n",
        "for word in words:\n",
        "    print(word,\"--->\",Porter.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_words = [\"walk\",\"walking\",\"walked\",\"walks\"]\n",
        "for word in new_words:\n",
        "    print(word,\"--->\",Porter.stem(word))"
      ],
      "metadata": {
        "id": "oupMbnhPXUA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f98115-36e2-4603-bf7f-47c4053db973"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walk ---> walk\n",
            "walking ---> walk\n",
            "walked ---> walk\n",
            "walks ---> walk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SnowballStemmer/Porter2Stemmer\n",
        "Designed and built by Martin Porter\n",
        "Advancement of PorterStemmer\n",
        "\n",
        "Faster and more precise than Porter Stemmer"
      ],
      "metadata": {
        "id": "U964E_O53SMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snowball = SnowballStemmer(language='english')\n",
        "\n",
        "for word in words:\n",
        "    print(word,\"--->\",snowball.stem(word))"
      ],
      "metadata": {
        "id": "9_XdGLfFoSr_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0182a38-f265-4c52-f254-07eb8ffd7117"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "friend ---> friend\n",
            "friendship ---> friendship\n",
            "friends ---> friend\n",
            "friendships ---> friendship\n",
            "generate ---> generat\n",
            "generates ---> generat\n",
            "generating ---> generat\n",
            "general ---> general\n",
            "generally ---> general\n",
            "generic ---> generic\n",
            "generically ---> generic\n",
            "generous ---> generous\n",
            "generously ---> generous\n",
            "went ---> went\n",
            "ate ---> ate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LancasterStemmer\n",
        "Simpler\n",
        "\n",
        "Results to over stemming of words, which leads to meaningless words"
      ],
      "metadata": {
        "id": "VNRbB3oD5TIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lancaster = LancasterStemmer()\n",
        "\n",
        "for word in words:\n",
        "    print(word,\"--->\",lancaster.stem(word))"
      ],
      "metadata": {
        "id": "KXm57JXP27mC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e05dc3e-1b62-443e-bed6-d5f2e5d81553"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "friend ---> friend\n",
            "friendship ---> friend\n",
            "friends ---> friend\n",
            "friendships ---> friend\n",
            "generate ---> gen\n",
            "generates ---> gen\n",
            "generating ---> gen\n",
            "general ---> gen\n",
            "generally ---> gen\n",
            "generic ---> gen\n",
            "generically ---> gen\n",
            "generous ---> gen\n",
            "generously ---> gen\n",
            "went ---> went\n",
            "ate ---> at\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RegexStemmer\n",
        "Uses regex\n",
        "\n",
        "Substring matching the regex will be discarded\n",
        "\n",
        "Worst performer"
      ],
      "metadata": {
        "id": "5B9i26p95cKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex = RegexpStemmer('ing$|s$|e$|able$|lly$|ate$', min=3)\n",
        "\n",
        "for word in words:\n",
        "    print(word,\"--->\",regex.stem(word))"
      ],
      "metadata": {
        "id": "kGwXP_lk5P6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmantizing"
      ],
      "metadata": {
        "id": "CQvja1ZC9pf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet = WordNetLemmatizer()\n",
        "lemm_word = ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
        "for word in lemm_word:\n",
        "    print(word,\"--->\",wordnet.lemmatize(word))"
      ],
      "metadata": {
        "id": "9gSWwpmz8ElG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize"
      ],
      "metadata": {
        "id": "2aGUc4uhGh-m"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import  pos_tag\n",
        "text = '''President Paul Kagame has said that deliberate efforts are needed to forge private-public partnerships to bridge internet usage gaps. He was speaking during the inauguration of the Mobile World Congress 2022 which convened more than 2000 people representing 99 countries, on October 25.\n",
        "Global mobile operators, device manufacturers, technology providers, vendors, content owners, and policymakers are in Kigali to identify gaps and discuss effective measures needed to drive digital transformation in Africa. To address the usage gap –the number of people who can’t use mobile internet services while living in an area covered by broadband networks –Kagame said that neither the private nor the public sector has all that is required to cover the gap, hence, the need for partnerships. '''\n",
        "\n",
        "tokens = list(tokenize(text))\n",
        "tokens"
      ],
      "metadata": {
        "id": "k7AVlGqB93oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_list = pos_tag(tokens)\n",
        "pos_list"
      ],
      "metadata": {
        "id": "Yl6KV33OF8zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matched_tags = {'NNP':'n',\"VBP\":'v'}\n",
        "processed_tag = []\n",
        "for token, tag in pos_list:\n",
        "  try:\n",
        "    token = wordnet.lemmatize(token,matched_tags[tag])\n",
        "    processed_tag.append(token)\n",
        "  except:\n",
        "    pass\n",
        "  #print(token,'-------------',tag)\n",
        "processed_tag"
      ],
      "metadata": {
        "id": "xj3N2oK7aFZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(wordnet.lemmatize('countries'))\n",
        "#pos_tag(['best'])\n",
        "print(wordnet.lemmatize('better','a'))"
      ],
      "metadata": {
        "id": "4eVyoDAUG3vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords\n",
        "Common simple words that add little value\n",
        "\n",
        "The goal is to reduce the size of the matrix as much as possible, therefore removing common words that do not add value makes sense. An example is I, a, an\n"
      ],
      "metadata": {
        "id": "O5hRNTOBIPMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "sw = stopwords.words('english')\n",
        "print(sw)"
      ],
      "metadata": {
        "id": "pXgd4po-HVpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11160032-0ed7-48aa-e925-7656ceedf5a0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)\n",
        "\n",
        "tokens_no_sw = []\n",
        "for token in tokens:\n",
        "  if token not in sw:\n",
        "    tokens_no_sw.append(token)\n",
        "\n",
        "print(len(tokens_no_sw))"
      ],
      "metadata": {
        "id": "9W8cv2tuRrqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5b71aa-865c-4982-8539-1f6dd7bcb55e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In-class practicls\n",
        "1. How many stop words are in NLTK, Spacy,Gensim. Compare them an select one\n",
        "2. Lemmantize the above text using a for loop\n",
        "3. Compare the Stemmers, get the best and compare in with Lemmantizer. \n",
        "4. Remove stop words from the text "
      ],
      "metadata": {
        "id": "s8DBaIAAJt1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# let's see the stopwords in nltk"
      ],
      "metadata": {
        "id": "uPncZk5BcnOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk_stopswords = stopwords.words('english')\n",
        "len(nltk_stopswords)"
      ],
      "metadata": {
        "id": "o6dx7FWWcti7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# let's see count of stopwords in spacy\n"
      ],
      "metadata": {
        "id": "Chcm0zA6dCTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "#loading the english language small model of spacy\n",
        "en = spacy.load('en_core_web_sm')\n",
        "stopwords = en.Defaults.stop_words\n",
        "\n",
        "print(len(stopwords))\n",
        "print(stopwords)"
      ],
      "metadata": {
        "id": "4UeQg0w5dHta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# let's see the stopswords in genism"
      ],
      "metadata": {
        "id": "MxI1q89mdjwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "gensim_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
        "len(gensim_stopwords)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDdokps0doV2",
        "outputId": "fa115ad6-3d3b-45b6-93f2-9d4bdfa9de1f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "337"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmantize the above text using a for loop"
      ],
      "metadata": {
        "id": "fhsKwsuliAoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet = WordNetLemmatizer()\n",
        "tokens_words___ = word_tokenize(text)\n",
        "for word in tokens_words___:\n",
        "    print(word,\"--->\",wordnet.lemmatize(word))"
      ],
      "metadata": {
        "id": "lUiT5vmtiDk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove the stopsword in the test I will be using spacy"
      ],
      "metadata": {
        "id": "lMajDuq3jwPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en = spacy.load('en_core_web_sm')\n",
        "stopwords_container = en.Defaults.stop_words\n",
        "final_words_wv = []\n",
        "for word in tokens_words___:\n",
        "  if word not in stopwords_container:\n",
        "    final_words_wv.append(word)\n",
        "final_words_wv"
      ],
      "metadata": {
        "id": "NOJkC4-Ej4oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "Create a function that takes the tokens, normalize the tokens and remove the stop words  "
      ],
      "metadata": {
        "id": "7B9p52D9KV_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def stemming_lem_sw (tokens):\n",
        "  new_tokens = []\n",
        "  for token in tokens:\n",
        "    token = snowball.stem(token)\n",
        "    if token not in sw:\n",
        "      new_tokens.append(token)\n",
        "\n",
        "  return new_tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "zTd5zyg0I8wQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}